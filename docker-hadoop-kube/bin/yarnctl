#!/usr/bin/env bash

set -ex

if [[ -z "${KUBE_NAMESPACE}" ]]
then
  KUBE_NAMESPACE="spark-lama-exps"
fi

if [[ -z "${REPO}" ]]
then
  REPO="node2.bdcl:5000"
fi

if [[ -z "${BASE_IMAGE_TAG}" ]]
then
  BASE_IMAGE_TAG="bookworm-20230227"
fi


if [[ -z "${IMAGE_TAG}" ]]
then
  IMAGE_TAG="slama-yarn-java-11-${BASE_IMAGE_TAG}"
fi

## declare an array variable
declare -a components=("base" "historyserver" "nodemanager" "resourcemanager" "jupyter-lab" "spark-history-server")

function obtain_requirements() {
  echo "Obtaining requirements files"

  cur_dir=$(pwd)

#  cd ../LightAutoML_Spark && poetry export --without-hashes -f requirements.txt > ${cur_dir}/nodemanager/requirements-3.9.txt && cd ${cur_dir}
#  cd ../LightAutoML_Spark_3.0.1 && poetry export --without-hashes -f requirements.txt > ${cur_dir}/nodemanager/requirements-3.8.txt && cd ${cur_dir}

#  cp ../LightAutoML_Spark/requirements-3.9.txt ./nodemanager
#  cp ../LightAutoML_Spark_3.0.1/requirements-3.8.txt ./nodemanager

  echo "Requirements files have been obtained"
}

function delete_requirements() {
    rm ../nodemanager/requirements*.txt
}

function build_images() {
    echo "Building required docker images..."

    obtain_requirements

    for comp in "${components[@]}"
    do
        image="${REPO}/yarn-${comp}:${IMAGE_TAG}"
        echo "Building docker image: ${image}"
        docker build --build-arg IMAGE_TAG=${IMAGE_TAG} --build-arg BASE_IMAGE_TAG=${BASE_IMAGE_TAG} \
          -t "${image}" "${comp}"
    done

    # TODO: add building
    # docker build --build-arg PYSPARK_VERSION=3.1.1 -t node2.bdcl:5000/yarn-spark-submit:slama-yarn-3.1.1 spark-submit

#    delete_requirements

    echo "The required images have been successfully built"
}

function push_images() {
    echo "Pushing required docker images into the repository ${REPO}"
    for comp in "${components[@]}"
    do
        image="${REPO}/yarn-${comp}:${IMAGE_TAG}"
        echo "Pushing docker image: ${image}"
        docker push "${image}"
    done
    echo "The required images have been successfully pushed"
}

function install_images() {
    build_images
    push_images
}

function build_jupyter_lab_image() {
    image="${REPO}/yarn-jupyter-lab:${IMAGE_TAG}"
    docker build -t "${image}" jupyter-lab
}

function push_jupyter_lab_image() {
    image="${REPO}/yarn-jupyter-lab:${IMAGE_TAG}"
    docker push "${image}"
}

function install_jupyter_lab_image() {
    build_jupyter_lab_image
    push_jupyter_lab_image
}

function provision_volumes() {
    kubectl -n ${KUBE_NAMESPACE} apply -f yarn-cluster-pv-pvc.yaml
}

function deploy() {
    kubectl -n ${KUBE_NAMESPACE} apply -f yarn-cluster.yaml
}

function teardown() {
    kubectl -n ${KUBE_NAMESPACE} delete -f yarn-cluster.yaml --ignore-not-found
#    TODO: actualize it later
#    kubectl -n ${KUBE_NAMESPACE} wait -f yarn-cluster.yaml --for=delete --timeout=60s
}

function teardown_workers() {
    kubectl -n ${KUBE_NAMESPACE} delete yarn-nodemanager --ignore-not-found
}

function restart_workers() {
    kubectl -n ${KUBE_NAMESPACE} rollout restart statefulset yarn-nodemanager
}

function redeploy() {
    teardown
    deploy
}

function scale() {
    kubectl -n ${KUBE_NAMESPACE} scale --replicas=$1 statefulset yarn-nodemanager
}

function build_spark_submit_image() {
    spark_version=$1
    docker build --build-arg PYSPARK_VERSION="${spark_version}" \
        -t "${REPO}/yarn-spark-submit:${IMAGE_TAG}-${spark_version}" spark-submit
}

function push_spark_submit_image() {
    spark_version=$1
    docker push "${REPO}/yarn-spark-submit:${IMAGE_TAG}-${spark_version}"
}

function install_spark_submit_image() {
    build_spark_submit_image "$1"
    push_spark_submit_image "$1"
}


function help() {
  echo "
  List of commands.
    build-images - builds docker images for components of the YARN cluster
    push-images - pushes built docker images into the repository to be used later in the kube cluster
    install-images - build and push images into the repository
    provision-volumes - create pv and pvc in the kubernetes cluster necessary for functioning of YARN
    deploy - deploys the YARN cluster
    teardown - destroys the YARN cluster
    redeploy - teardown and then deploy
    teardown-workers - destroy only node managers
    restart-workers - make a rollout restart of node managers
    scale - increase or decrease the number of node managers (argument: a new number of node managers)
    build-spark-submit - build a spark-submit image that is used for testing purposes (argument: spark version)
    push-spark-submit - push the built spark-submit image to the repo (argument: spark version)
    install-spark-submit - build and push the spark-submit image (argument: spark version)
    help - prints this message
  "
}

function main () {
    cmd="$1"

    if [ -z "${cmd}" ]
    then
      echo "No command is provided."
      help
      exit 1
    fi

    shift 1

    echo "Executing command: ${cmd}"

    case "${cmd}" in
    "obtain-reqs")
        obtain_requirements
        ;;

    "build-images")
        build_images
        ;;

    "push-images")
        push_images
        ;;

    "install-images")
        install_images
        ;;

    "build-jupyter-lab-image")
        build_jupyter_lab_image
        ;;

    "push-jupyter-lab-image")
        push_jupyter_lab_image
        ;;

    "install-jupyter-lab-image")
        install_jupyter_lab_image
        ;;

    "provision-volumes")
        provision_volumes
        ;;

    "deploy")
        deploy
        ;;

    "teardown")
        teardown
        ;;

    "redeploy")
        redeploy
        ;;

    "teardown-workers")
        teardown_workers
        ;;

    "restart-workers")
        restart_workers
        ;;

    "scale")
        scale "${@}"
        ;;

    "build-spark-submit")
        build_spark_submit_image "${@}"
        ;;

    "push-spark-submit")
        push_spark_submit_image "${@}"
        ;;

    "install-spark-submit")
        install_spark_submit_image "${@}"
        ;;

    "help")
        help
        ;;

    *)
        echo "Unknown command: ${cmd}"
        ;;

    esac
}

main "${@}"
