#!/usr/bin/env bash

set -ex

# uncomment to reddefine spark-submit command for debug purposes
#spark-submit()
#{
#    echo "Calling spark-submit"
#}

if [[ -z "$1" ]]; then
  echo "SCRIPT arg is not defined but required"
  exit 1
fi

if [[ -z "$EXPERIMENT" ]]; then
  echo "EXPERIMENT env var is not defined but required"
  exit 1
fi

SCRIPT="$1"

#PYSPARK_PYTHON_PATH=${PYSPARK_PYTHON_PATH:-"/python_envs/Python-3.9.12/bin/python3.9"}
PYSPARK_PYTHON_PATH=${PYSPARK_PYTHON_PATH:-"/python_envs/.replay_venv/bin/python3.9"}

MAX_JOB_PARALLELISM=${MAX_JOB_PARALLELISM:-"3"}
PREPARE_FOLD_NUM=${PREPARE_FOLD_NUM:-"5"}
USE_FOLD_NUM=${USE_FOLD_NUM:-"5"}
PARALLELISM_MODE=${PARALLELISM_MODE:-"pref_locs"}
LOG_TO_MLFLOW=${LOG_TO_MLFLOW:-"1"}
LOG_FILES_TO_MLFLOW=${LOG_FILES_TO_MLFLOW:-"0"}
DATASET=${DATASET:-"lama_test_dataset"}

SEED=${SEED:-"42"}
CV=${CV:-"5"}
PERSISTENCE_MANAGER=${PERSISTENCE_MANAGER:-"CompositeBucketedPersistenceManager"}
DRIVER_CORES=${DRIVER_CORES:-"6"}
DRIVER_MEMORY=${DRIVER_MEMORY:-"16g"}
DRIVER_MAX_RESULT_SIZE=${DRIVER_MAX_RESULT_SIZE:-"5g"}
EXECUTOR_INSTANCES=${EXECUTOR_INSTANCES:-"4"}
EXECUTOR_CORES=${EXECUTOR_CORES:-"6"}
EXECUTOR_MEMORY=${EXECUTOR_MEMORY:-"40g"}
PART_NUM_KOEF=${PART_NUM_KOEF:-1}
WAREHOUSE_DIR=${WAREHOUSE_DIR:-"hdfs://node21.bdcl:9000/slama-spark-warehouse"}

# calculable variables
# shellcheck disable=SC2004
CORES_MAX=$(($EXECUTOR_CORES * $EXECUTOR_INSTANCES))
# shellcheck disable=SC2004
PARTITION_NUM=$(($CORES_MAX * $PART_NUM_KOEF))
BUCKET_NUMS=${BUCKET_NUMS:-$PARTITION_NUM}
MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-"http://node2.bdcl:8811"}

export SPARK_NO_DAEMONIZE=1

exp_vars=$(printenv | grep '^EXP_' | awk '{print "--conf spark.yarn.appMasterEnv."$1}' | tr '\n' ' ')

#--conf "spark.executor.memoryOverhead=40800" \
# we intentionally remove this property
#--conf "spark.default.parallelism=${PARTITION_NUM}" \
#--conf "spark.jars.packages=ai.catboost:catboost-spark_3.2_2.12:1.2" \
spark-submit \
--master yarn \
--deploy-mode cluster \
--conf "spark.yarn.appMasterEnv.SCRIPT_ENV=cluster" \
--conf "spark.yarn.appMasterEnv.PYSPARK_PYTHON=${PYSPARK_PYTHON_PATH}" \
--conf "spark.yarn.appMasterEnv.MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}" \
--conf "spark.yarn.appMasterEnv.MAX_JOB_PARALLELISM=${MAX_JOB_PARALLELISM}" \
--conf "spark.yarn.appMasterEnv.PREPARE_FOLD_NUM=${PREPARE_FOLD_NUM}" \
--conf "spark.yarn.appMasterEnv.USE_FOLD_NUM=${USE_FOLD_NUM}" \
--conf "spark.yarn.appMasterEnv.PARALLELISM_MODE=${PARALLELISM_MODE}" \
--conf "spark.yarn.appMasterEnv.LOG_TO_MLFLOW=${LOG_TO_MLFLOW}" \
--conf "spark.yarn.appMasterEnv.LOG_FILES_TO_MLFLOW=${LOG_FILES_TO_MLFLOW}" \
--conf "spark.yarn.appMasterEnv.GIT_PYTHON_REFRESH=quiet" \
--conf "spark.yarn.appMasterEnv.EXPERIMENT=${EXPERIMENT}" \
--conf "spark.yarn.appMasterEnv.DATASET=${DATASET}" \
--conf "spark.yarn.appMasterEnv.SEED=${SEED}" \
--conf "spark.yarn.appMasterEnv.SEED=${CV}" \
--conf "spark.yarn.appMasterEnv.PERSISTENCE_MANAGER=${PERSISTENCE_MANAGER}" \
--conf "spark.yarn.appMasterEnv.BUCKET_NUMS=${BUCKET_NUMS}" \
--conf "spark.yarn.appMasterEnv.LGB_NUM_TASKS=${LGB_NUM_TASKS}" \
--conf "spark.yarn.appMasterEnv.LGB_NUM_THREADS=${LGB_NUM_THREADS}" \
${exp_vars} \
--conf "spark.yarn.tags=slama" \
--conf "spark.kryoserializer.buffer.max=512m" \
--conf "spark.driver.cores=${DRIVER_CORES}" \
--conf "spark.driver.memory=${DRIVER_MEMORY}" \
--conf "spark.driver.maxResultSize=${DRIVER_MAX_RESULT_SIZE}" \
--conf "spark.executor.instances=${EXECUTOR_INSTANCES}" \
--conf "spark.executor.cores=${EXECUTOR_CORES}" \
--conf "spark.executor.memory=${EXECUTOR_MEMORY}" \
--conf "spark.cores.max=${CORES_MAX}" \
--conf "spark.memory.fraction=0.8" \
--conf "spark.sql.shuffle.partitions=${PARTITION_NUM}" \
--conf "spark.yarn.maxAppAttempts=1" \
--conf "spark.yarn.am.waitTime=200s" \
--conf "spark.rpc.message.maxSize=1024" \
--conf "spark.sql.autoBroadcastJoinThreshold=100MB" \
--conf "spark.sql.execution.arrow.pyspark.enabled=true" \
--conf "spark.scheduler.minRegisteredResourcesRatio=1.0" \
--conf "spark.scheduler.maxRegisteredResourcesWaitingTime=180s" \
--conf "spark.eventLog.enabled=true" \
--conf "spark.task.cpus=4" \
--conf "spark.eventLog.dir=hdfs://node21.bdcl:9000/shared/spark-logs" \
--conf "spark.yarn.historyServer.allowTracking=true" \
--conf "spark.driver.extraJavaOptions=-Dio.netty.tryReflectionSetAccessible=true -Dlog4j.configuration=log4j.properties -Dlog4j.debug=true" \
--conf "spark.executor.extraJavaOptions=-Dio.netty.tryReflectionSetAccessible=true -Dlog4j.configuration=log4j.properties -Dlog4j.debug=true" \
--conf "spark.sql.warehouse.dir=${WAREHOUSE_DIR}" \
--conf "spark.task.maxFailures=1" \
--conf "spark.excludeOnFailure.task.maxTaskAttemptsPerNode=1" \
--conf "spark.excludeOnFailure.stage.maxFailedTasksPerExecutor=1" \
--conf "spark.excludeOnFailure.stage.maxFailedExecutorsPerNode=1" \
--conf "spark.excludeOnFailure.application.maxFailedTasksPerExecutor=1" \
--conf "spark.excludeOnFailure.application.maxFailedExecutorsPerNode=1" \
--conf "spark.jars.packages=ai.catboost:catboost-spark_3.2_2.12:1.2" \
--files "/src/log4j.properties" \
--py-files "/src/SparkLightAutoML-${SLAMA_WHEEL_VERSION}-py3-none-any.whl,/src/examples-spark/*,/src/tabular_config.yml" \
--num-executors "${EXECUTOR_INSTANCES}" \
--jars "/src/spark-lightautoml_2.12-${SLAMA_JAR_VERSION}.jar" \
"${SCRIPT}"

#--conf "spark.executor.extraClassPath=/jars/catboost_3.2_2.12_1.2_jars/*" \
#--conf "spark.driver.extraClassPath=/jars/catboost_3.2_2.12_1.2_jars/*" \
