#!/usr/bin/env bash

set -ex

if [[ -z "$1" ]]; then
  echo "SCRIPT arg is not defined but required"
  exit 1
fi

if [[ -z "$EXPERIMENT" ]]; then
  echo "EXPERIMENT env var is not defined but required"
  exit 1
fi

SCRIPT="$1"

PYSPARK_PYTHON_PATH=${PYSPARK_PYTHON_PATH:-"/python_envs/Python-3.9.12/bin/python3.9"}
LOG_TO_MLFLOW=${LOG_TO_MLFLOW:-"1"}
LOG_FILES_TO_MLFLOW=${LOG_FILES_TO_MLFLOW:-"0"}
DATASET=${DATASET:-"lama_test_dataset"}
SEED=${SEED:-"42"}
CV=${CV:-"5"}
PARALLELISM=${PARALLELISM:-"1"}
PERSISTENCE_MANAGER=${PERSISTENCE_MANAGER:-"CompositeBucketedPersistenceManager"}
DRIVER_CORES=${DRIVER_CORES:-"6"}
DRIVER_MEMORY=${DRIVER_MEMORY:-"16g"}
DRIVER_MAX_RESULT_SIZE=${DRIVER_MAX_RESULT_SIZE:-"5g"}
EXECUTOR_INSTANCES=${EXECUTOR_INSTANCES:-"4"}
EXECUTOR_CORES=${EXECUTOR_CORES:-"6"}
EXECUTOR_MEMORY=${EXECUTOR_MEMORY:-"40g"}
PART_NUM_KOEF=${PART_NUM_KOEF:-1}
WAREHOUSE_DIR=${WAREHOUSE_DIR:-"hdfs://node21.bdcl:9000/slama-spark-warehouse"}

# calculable variables
# shellcheck disable=SC2004
CORES_MAX=$(($EXECUTOR_CORES * $EXECUTOR_INSTANCES))
# shellcheck disable=SC2004
PARTITION_NUM=$(($CORES_MAX * $PART_NUM_KOEF))
BUCKET_NUMS=${BUCKET_NUMS:-$PARTITION_NUM}

export SPARK_NO_DAEMONIZE=1

spark-submit \
--master yarn \
--deploy-mode cluster \
--conf "spark.yarn.appMasterEnv.SCRIPT_ENV=cluster" \
--conf "spark.yarn.appMasterEnv.PARALLELISM=${PARALLELISM}" \
--conf "spark.yarn.appMasterEnv.PYSPARK_PYTHON=${PYSPARK_PYTHON_PATH}" \
--conf "spark.yarn.appMasterEnv.MLFLOW_TRACKING_URI=http://node2.bdcl:8811" \
--conf "spark.yarn.appMasterEnv.LOG_TO_MLFLOW=${LOG_TO_MLFLOW}" \
--conf "spark.yarn.appMasterEnv.LOG_FILES_TO_MLFLOW=${LOG_FILES_TO_MLFLOW}" \
--conf "spark.yarn.appMasterEnv.GIT_PYTHON_REFRESH=quiet" \
--conf "spark.yarn.appMasterEnv.EXPERIMENT=${EXPERIMENT}" \
--conf "spark.yarn.appMasterEnv.DATASET=${DATASET}" \
--conf "spark.yarn.appMasterEnv.SEED=${SEED}" \
--conf "spark.yarn.appMasterEnv.SEED=${CV}" \
--conf "spark.yarn.appMasterEnv.PERSISTENCE_MANAGER=${PERSISTENCE_MANAGER}" \
--conf "spark.yarn.appMasterEnv.BUCKET_NUMS=${BUCKET_NUMS}" \
--conf "spark.yarn.tags=slama" \
--conf "spark.kryoserializer.buffer.max=512m" \
--conf "spark.driver.cores=${DRIVER_CORES}" \
--conf "spark.driver.memory=${DRIVER_MEMORY}" \
--conf "spark.driver.maxResultSize=${DRIVER_MAX_RESULT_SIZE}" \
--conf "spark.executor.instances=${EXECUTOR_INSTANCES}" \
--conf "spark.executor.cores=${EXECUTOR_CORES}" \
--conf "spark.executor.memory=${EXECUTOR_MEMORY}" \
--conf "spark.cores.max=${CORES_MAX}" \
--conf "spark.memory.fraction=0.8" \
--conf "spark.sql.shuffle.partitions=${PARTITION_NUM}" \
--conf "spark.default.parallelism=${PARTITION_NUM}" \
--conf "spark.yarn.maxAppAttempts=1" \
--conf "spark.rpc.message.maxSize=1024" \
--conf "spark.sql.autoBroadcastJoinThreshold=100MB" \
--conf "spark.sql.execution.arrow.pyspark.enabled=true" \
--conf "spark.scheduler.minRegisteredResourcesRatio=1.0" \
--conf "spark.scheduler.maxRegisteredResourcesWaitingTime=180s" \
--conf "spark.eventLog.enabled=true" \
--conf "spark.eventLog.dir=hdfs://node21.bdcl:9000/shared/spark-logs" \
--conf "spark.yarn.historyServer.allowTracking=true" \
--conf "spark.driver.extraJavaOptions=-Dio.netty.tryReflectionSetAccessible=true" \
--conf "spark.executor.extraJavaOptions=-Dio.netty.tryReflectionSetAccessible=true" \
--conf "spark.executor.extraClassPath=/jars/synapseml_0.9.5_jars/*" \
--conf "spark.driver.extraClassPath=/jars/synapseml_0.9.5_jars/*" \
--conf "spark.sql.warehouse.dir=${WAREHOUSE_DIR}" \
--conf "spark.task.maxFailures=1" \
--conf "spark.excludeOnFailure.task.maxTaskAttemptsPerNode=1" \
--conf "spark.excludeOnFailure.stage.maxFailedTasksPerExecutor=1" \
--conf "spark.excludeOnFailure.stage.maxFailedExecutorsPerNode=1" \
--conf "spark.excludeOnFailure.application.maxFailedTasksPerExecutor=1" \
--conf "spark.excludeOnFailure.application.maxFailedExecutorsPerNode=1" \
--py-files "/src/SparkLightAutoML_DEV-0.3.0-py3-none-any.whl,/src/examples-spark/*,/src/tabular_config.yml" \
--num-executors "${EXECUTOR_INSTANCES}" \
--jars "/src/spark-lightautoml_2.12-0.1.jar" \
"${SCRIPT}"
